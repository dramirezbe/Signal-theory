{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import scipy.signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_matrix(path):\n",
    "    fs_v = []\n",
    "    audio_m = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        \n",
    "        \n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        y = librosa.to_mono(y)\n",
    "        y = y.reshape(-1, 1)\n",
    "        y = y.flatten()\n",
    "        \n",
    "        fs_v.append(sr)\n",
    "        audio_m.append(y)\n",
    "\n",
    "    fs = np.mean(fs_v)\n",
    "\n",
    "    \n",
    "    max_length = max(len(y) for y in audio_m)\n",
    "    audio_m_padded = np.array([np.pad(y, (0, max_length - len(y)), 'constant') for y in audio_m])\n",
    "\n",
    "    return audio_m_padded, fs\n",
    "\n",
    "def welch_vector(X, fs, window, nperseg, nfft):\n",
    "    return sig.welch(X, fs, window=window, nperseg=nperseg, noverlap=(nperseg//2), nfft=nfft)\n",
    "\n",
    "def psd_matrix(audio_m,  fs, window):\n",
    "    M = audio_m.shape[0]\n",
    "    N = audio_m.shape[1]\n",
    "\n",
    "    psd_matrix = np.zeros((M,N))          \n",
    "    for i in range(M):\n",
    "        audio_v = audio_m[i]\n",
    "        f, psd = welch_vector(audio_v, fs,  window, (N-1), (N*2)-1)\n",
    "        psd_matrix[i] = psd\n",
    "    return f, psd_matrix\n",
    "\n",
    "def audio_classif(classif,  audio_m):\n",
    "    N = audio_m.shape[1]\n",
    "\n",
    "    one_count = np.sum(classif)\n",
    "    zero_count = len(classif) - one_count\n",
    "\n",
    "    audio_one_m = np.zeros((one_count, N))\n",
    "    audio_zero_m = np.zeros((zero_count, N))\n",
    "\n",
    "    audio_one_m = audio_m[classif == 1]\n",
    "    audio_zero_m = audio_m[classif == 0]\n",
    "\n",
    "    return  audio_one_m, audio_zero_m\n",
    "\n",
    "def firwin_filter(signal, lowcut, highcut, numtaps, fs): #Filter FIR\n",
    "\n",
    "    filter_coeficients = sig.firwin(numtaps, [lowcut, highcut], pass_zero=False, fs=fs)\n",
    "\n",
    "    return sig.lfilter(filter_coeficients, 1.0, signal)\n",
    "\n",
    "def audio_filter(audio_m, lowcut, highcut, numtaps, fs):\n",
    "    M = audio_m.shape[0]\n",
    "    N = audio_m.shape[1]\n",
    "\n",
    "    filtered_signal_m = np.zeros((M,N))\n",
    "    for i in range(M):\n",
    "        filtered_signal_m[i] = firwin_filter(audio_m[i], lowcut, highcut, numtaps, fs)\n",
    "    return filtered_signal_m\n",
    "\n",
    "\n",
    "def spectre_to_image(fs, signal_m):\n",
    "    image_spec = []\n",
    "\n",
    "    for signal_v in signal_m:  \n",
    "        \n",
    "        _, _, Sxx = sig.spectrogram(signal_v, fs)\n",
    "\n",
    "        Sxx_dB = 10 * np.log10(Sxx)\n",
    "\n",
    "        Sxx_normalized = 255 * (Sxx_dB - np.min(Sxx_dB)) / (np.max(Sxx_dB) - np.min(Sxx_dB))\n",
    "        Sxx_grayscale = Sxx_normalized.astype(np.uint8)\n",
    "\n",
    "        img_3d = np.expand_dims(Sxx_grayscale, axis=-1)\n",
    "\n",
    "        image_spec.append(img_3d)\n",
    "    image_spec = np.array(image_spec)\n",
    "\n",
    "    return image_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_names</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0_0_.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0_1_.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0_2_.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0_3_.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0_4_.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    audio_names  Classification\n",
       "0  100.0_0_.wav               0\n",
       "1  100.0_1_.wav               0\n",
       "2  100.0_2_.wav               0\n",
       "3  100.0_3_.wav               1\n",
       "4  100.0_4_.wav               0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\Audio_fm'\n",
    "audio_m, fs = audio_matrix(path)\n",
    "df = pd.read_excel('audio_fm_classification.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 30\n",
    "highcut = 15000\n",
    "numtaps = 301\n",
    "audio_m_filter = audio_filter(audio_m, lowcut, highcut, numtaps, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = df['Classification'].to_numpy()\n",
    "\n",
    "audio_img = spectre_to_image(fs, audio_m_filter)\n",
    "\n",
    "np.save('audio_img.npy', audio_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356, 83886) (849, 83886)\n"
     ]
    }
   ],
   "source": [
    "audio_ones, audio_zeros = audio_classif(classif, audio_m_filter)\n",
    "\n",
    "print(audio_ones.shape, audio_zeros.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
